## Connexion au serveur

Lancer Putty depuis le menu démarrer.
**Host Name :** clust-n7

Dans la console qui s'afffiche, renseigner votre identifiant et mot de passe habituel.

Spark se lance en exécutant la commande :

- `/etc/spark/bin/spark-shell` (pour l'interface en Scala, l'interface native)
- `/etc/spark/bin/pyspark` (pour l'interface en Python)
- `/etc/spark/bin/sparkR`  (pour l'interface en R)

Les données sont accessibles ......... . Les données originales viennent de.

## Exercice 1: lecture des données


**Pour patienter:** refaire les exercices en Python et en R

## Exercice 2: le principe map-reduce

**Pour patienter:** refaire les exercices en Python et en R

## Exercice 3: applications

**Pour patienter:** refaire les exercices en Python et en R





## Pour approfondir/ réviser:

- Une série de billets introductifs en français:
    1. https://aseigneurin.github.io/2014/10/29/introduction-apache-spark.html
    2. https://aseigneurin.github.io/2014/11/01/initiation-mapreduce-avec-apache-spark.html
    3. https://aseigneurin.github.io/2014/11/06/mapreduce-et-manipulations-par-cles-avec-apache-spark.html
    
- "Quick start", sur le site officiel de Spark: https://spark.apache.org/docs/latest/quick-start.html